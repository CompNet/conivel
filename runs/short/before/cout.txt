INFO - xp_kfolds_list - Running command 'main'
INFO - xp_kfolds_list - Started run with ID "4"
Configuration ([34mmodified[0m, [32madded[0m, [31mtypechanged[0m, [2mdoc[0m):
[32m  batch_size = 8[0m
  k = 5
  ner_epochs_nb = 2
  ner_lr = 2e-05
[32m  retrievers_names = ['left'][0m
[34m  runs_nb = 3[0m
[34m  save_models = False[0m
  seed = 227933120                   [2m# the random seed for this experiment[0m
[32m  sents_nb_list = [1, 2, 3, 4, 5, 6][0m
[34m  shuffle_kfolds_seed = 0[0m
  __annotations__:
    batch_size = <class 'int'>
    k = <class 'int'>
    ner_epochs_nb = <class 'int'>
    ner_lr = <class 'float'>
    retrievers_names = <class 'list'>
    runs_nb = <class 'int'>
    save_models = <class 'bool'>
    sents_nb_list = <class 'list'>
    shuffle_kfolds_seed = typing.Union[int, NoneType]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:00<00:00, 459.88it/s]
Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loss : 0.342:   1%|          | 14/1276 [00:01<01:46, 11.86it/s]]]
epoch mean loss : 0.036
  1%|          | 10/1276 [00:00<01:20, 15.64it/s]]]
{
    "validation precision": 0.9935911602209945,
    "validation recall": 0.9949103784023013,
    "validation f1": 0.994250331711632
}
loss : 0.007:   4%|â–         | 56/1276 [00:09<03:10,  6.40it/s]]]
epoch mean loss : 0.003
  0%|          | 4/1276 [00:00<00:40, 31.31it/s]s]]
{
    "validation precision": 0.9955840141311548,
    "validation recall": 0.9977871210444789,
    "validation f1": 0.9966843501326261
}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 789.39it/s]
 19%|â–ˆâ–‰        | 56/296 [00:01<00:12, 19.80it/s]]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 682.88it/s]
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 176/296 [00:09<00:06, 18.80it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 597.17it/s]
 21%|â–ˆâ–ˆ        | 62/296 [00:04<00:23,  9.90it/s]]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 547.26it/s]
 17%|â–ˆâ–‹        | 51/296 [00:03<00:20, 12.20it/s]]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 483.76it/s]
 25%|â–ˆâ–ˆâ–       | 73/296 [00:08<01:28,  2.51it/s]]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 421.71it/s]
  1%|          | 3/296 [00:00<00:20, 14.10it/s]]]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:00<00:00, 469.80it/s]
Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loss : 0.259:   3%|â–Ž         | 33/1300 [00:05<02:35,  8.17it/s]]]
epoch mean loss : 0.035
  2%|â–         | 31/1300 [00:01<00:55, 22.93it/s]]]
{
    "validation precision": 0.9844890510948905,
    "validation recall": 0.9890009165902841,
    "validation f1": 0.9867398262459991
}
loss : 0.011:   0%|          | 2/1300 [00:00<02:04, 10.45it/s]]]]
epoch mean loss : 0.004
  4%|â–         | 50/1300 [00:02<00:54, 22.96it/s]]]
{
    "validation precision": 0.9947332264712617,
    "validation recall": 0.995417048579285,
    "validation f1": 0.9950750200435231
}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 852.37it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 273/273 [00:05<00:00, 49.39it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 740.54it/s]
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 116/273 [00:03<00:03, 40.74it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 634.96it/s]
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 211/273 [00:06<00:01, 37.51it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 594.13it/s]
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 243/273 [00:08<00:00, 31.67it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 512.82it/s]
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 229/273 [00:09<00:01, 26.30it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 467.85it/s]
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 192/273 [00:08<00:03, 26.81it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:00<00:00, 194.52it/s]
Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loss : 0.104:   4%|â–Ž         | 45/1242 [00:04<01:39, 11.97it/s]]]
epoch mean loss : 0.040
 10%|â–ˆ         | 125/1242 [00:09<02:28,  7.50it/s]]
{
    "validation precision": 0.9944903581267218,
    "validation recall": 0.9932275132275132,
    "validation f1": 0.9938585345192716
}
loss : 0.001:   1%|          | 8/1242 [00:01<02:53,  7.10it/s]]]]
epoch mean loss : 0.003
 14%|â–ˆâ–        | 172/1242 [00:08<00:40, 26.74it/s]]
{
    "validation precision": 0.9976749101669837,
    "validation recall": 0.9989417989417989,
    "validation f1": 0.9983079526226734
}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 64.72it/s]
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 256/331 [00:06<00:02, 31.37it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 636.25it/s]
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 160/331 [00:05<00:04, 34.54it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 547.97it/s]
  6%|â–Œ         | 20/331 [00:00<00:14, 21.38it/s]]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 476.56it/s]
  1%|          | 3/331 [00:00<00:14, 22.00it/s]s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 430.60it/s]
 27%|â–ˆâ–ˆâ–‹       | 90/331 [00:05<00:18, 13.00it/s]]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 388.65it/s]
 26%|â–ˆâ–ˆâ–Œ       | 86/331 [00:09<00:41,  5.86it/s]]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:00<00:00, 200.05it/s]
Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loss : 0.477:   2%|â–         | 20/1186 [00:03<03:35,  5.40it/s]]]
epoch mean loss : 0.038
  0%|          | 1/1186 [00:00<02:36,  7.59it/s]s]]
{
    "validation precision": 0.9895984518626028,
    "validation recall": 0.9934434191355027,
    "validation f1": 0.991517207949588
}
loss : 0.000:   4%|â–         | 51/1186 [00:08<02:55,  6.46it/s]]]
epoch mean loss : 0.002
  6%|â–Œ         | 73/1186 [00:03<01:20, 13.76it/s]]]
{
    "validation precision": 0.9975716367168529,
    "validation recall": 0.9975716367168529,
    "validation f1": 0.9975716367168529
}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 63.65it/s]
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 215/387 [00:04<00:03, 53.03it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 537.99it/s]
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 267/387 [00:06<00:02, 40.33it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 466.72it/s]
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 257/387 [00:07<00:03, 41.19it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 417.07it/s]
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 194/387 [00:06<00:05, 36.67it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 362.98it/s]
 22%|â–ˆâ–ˆâ–       | 86/387 [00:03<00:12, 23.49it/s]]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 59.38it/s]
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 241/387 [00:09<00:05, 24.51it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:00<00:00, 468.51it/s]
Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loss : 0.329:   1%|â–         | 19/1286 [00:01<01:58, 10.72it/s]]]
epoch mean loss : 0.037
  6%|â–Œ         | 78/1286 [00:08<02:56,  6.84it/s]]]
{
    "validation precision": 0.9841409691629956,
    "validation recall": 0.9871851524524967,
    "validation f1": 0.9856607103463491
}
loss : 0.016:   4%|â–Ž         | 48/1286 [00:09<04:01,  5.12it/s]]]
epoch mean loss : 0.003
  4%|â–         | 56/1286 [00:05<03:33,  5.76it/s]]]
{
    "validation precision": 0.9911777679752978,
    "validation recall": 0.9929297392841361,
    "validation f1": 0.9920529801324504
}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 790.65it/s]
 31%|â–ˆâ–ˆâ–ˆ       | 89/287 [00:02<00:07, 26.45it/s]]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 697.15it/s]
 10%|â–ˆ         | 29/287 [00:01<00:12, 21.32it/s]]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 619.82it/s]
 31%|â–ˆâ–ˆâ–ˆ       | 89/287 [00:05<00:34,  5.78it/s]]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 550.68it/s]
 34%|â–ˆâ–ˆâ–ˆâ–      | 98/287 [00:07<00:28,  6.57it/s]]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 496.67it/s]
 26%|â–ˆâ–ˆâ–‹       | 76/287 [00:05<00:19, 10.90it/s]]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 445.74it/s]
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 178/287 [00:07<00:04, 25.95it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:00<00:00, 191.94it/s]
Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loss : 0.336:   2%|â–         | 21/1276 [00:02<02:05,  9.98it/s]]]
epoch mean loss : 0.037
  9%|â–‰         | 114/1276 [00:05<00:46, 25.18it/s]]
{
    "validation precision": 0.9942477876106195,
    "validation recall": 0.9944678026111972,
    "validation f1": 0.9943577829405907
}
loss : 0.018:   1%|          | 13/1276 [00:01<04:01,  5.22it/s]]]
epoch mean loss : 0.003
  1%|          | 8/1276 [00:00<00:41, 30.73it/s]s]]
{
    "validation precision": 0.9982300884955753,
    "validation recall": 0.9984509847311352,
    "validation f1": 0.9983405243942913
}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 787.03it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 296/296 [00:06<00:00, 45.45it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 678.26it/s]
 22%|â–ˆâ–ˆâ–       | 65/296 [00:01<00:08, 27.77it/s]]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 606.33it/s]
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 97/296 [00:03<00:06, 28.63it/s]]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 531.02it/s]
 19%|â–ˆâ–‰        | 57/296 [00:02<00:21, 11.31it/s]]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 485.09it/s]
 20%|â–ˆâ–ˆ        | 60/296 [00:04<00:24,  9.69it/s]]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 433.03it/s]
 12%|â–ˆâ–Ž        | 37/296 [00:02<00:26,  9.83it/s]]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:00<00:00, 188.28it/s]
Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loss : 0.130:   6%|â–Œ         | 73/1300 [00:09<02:48,  7.28it/s]]]
epoch mean loss : 0.038
  9%|â–‰         | 120/1300 [00:06<01:05, 17.92it/s]]
{
    "validation precision": 0.9906114037096405,
    "validation recall": 0.9912923923006416,
    "validation f1": 0.9909517810101935
}
loss : 0.012:   3%|â–Ž         | 37/1300 [00:03<02:17,  9.22it/s]]]
epoch mean loss : 0.003
  6%|â–Œ         | 77/1300 [00:06<02:09,  9.45it/s]]]
{
    "validation precision": 0.9947416552354824,
    "validation recall": 0.9970210815765352,
    "validation f1": 0.9958800640878919
}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 846.84it/s]
 10%|â–‰         | 27/273 [00:00<00:06, 39.55it/s]]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 721.32it/s]
 14%|â–ˆâ–        | 39/273 [00:01<00:14, 16.61it/s]]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 646.33it/s]
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 97/273 [00:07<00:11, 15.13it/s]]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 562.85it/s]
  6%|â–Œ         | 16/273 [00:00<00:17, 14.61it/s]]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 514.24it/s]
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 96/273 [00:09<00:35,  5.03it/s]]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 468.83it/s]
 16%|â–ˆâ–‹        | 45/273 [00:06<00:28,  8.01it/s]]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:00<00:00, 189.99it/s]
Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loss : 0.179:   2%|â–         | 30/1242 [00:06<08:58,  2.25it/s]]]
epoch mean loss : 0.039
  7%|â–‹         | 81/1242 [00:04<01:05, 17.86it/s]]]
{
    "validation precision": 0.9915522703273495,
    "validation recall": 0.9936507936507937,
    "validation f1": 0.9926004228329811
}
loss : 0.000:   3%|â–Ž         | 32/1242 [00:03<02:36,  7.74it/s]]]
epoch mean loss : 0.003
  9%|â–‰         | 111/1242 [00:05<01:04, 17.65it/s]]
{
    "validation precision": 0.997884493336154,
    "validation recall": 0.9983068783068783,
    "validation f1": 0.9980956411341515
}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 65.06it/s]
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 248/331 [00:04<00:01, 41.73it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 624.75it/s]
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 314/331 [00:07<00:00, 32.47it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 533.27it/s]
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 317/331 [00:09<00:00, 24.24it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 458.29it/s]
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 287/331 [00:09<00:02, 21.71it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 421.53it/s]
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 224/331 [00:07<00:04, 22.98it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 384.58it/s]
 32%|â–ˆâ–ˆâ–ˆâ–      | 107/331 [00:04<00:11, 18.85it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:00<00:00, 196.50it/s]
Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loss : 0.161:   6%|â–‹         | 77/1186 [00:07<01:41, 10.95it/s]]]
epoch mean loss : 0.037
  7%|â–‹         | 84/1186 [00:04<01:10, 15.72it/s]]]
{
    "validation precision": 0.9871202916160389,
    "validation recall": 0.986401165614376,
    "validation f1": 0.9867605975950444
}
loss : 0.000:   4%|â–Ž         | 42/1186 [00:07<06:37,  2.87it/s]]]
epoch mean loss : 0.003
 11%|â–ˆ         | 125/1186 [00:06<00:59, 17.74it/s]]
{
    "validation precision": 0.9917555771096024,
    "validation recall": 0.9932005828071879,
    "validation f1": 0.9924775539917495
}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 63.69it/s]
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 361/387 [00:07<00:00, 59.11it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 521.39it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 387/387 [00:08<00:00, 43.19it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 473.55it/s]
  0%|          | 0/387 [00:00<?, ?it/s]42.98it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 410.20it/s]
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 282/387 [00:09<00:03, 32.00it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 358.50it/s]
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 179/387 [00:06<00:06, 30.44it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 58.02it/s]
 13%|â–ˆâ–Ž        | 49/387 [00:02<00:20, 16.22it/s]]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:00<00:00, 456.44it/s]
Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loss : 0.144:   4%|â–Ž         | 48/1286 [00:04<01:59, 10.35it/s]]]
epoch mean loss : 0.038
  6%|â–Œ         | 72/1286 [00:06<03:10,  6.38it/s]]]
{
    "validation precision": 0.9880557398805574,
    "validation recall": 0.986964206805126,
    "validation f1": 0.9875096717143805
}
loss : 0.000:   3%|â–Ž         | 40/1286 [00:06<02:50,  7.33it/s]]]
epoch mean loss : 0.003
  3%|â–Ž         | 38/1286 [00:01<01:21, 15.29it/s]]]
{
    "validation precision": 0.9949249779346867,
    "validation recall": 0.9962439239946973,
    "validation f1": 0.9955840141311547
}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 841.17it/s]
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 202/287 [00:06<00:03, 26.80it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 664.69it/s]
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 129/287 [00:06<00:08, 18.57it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 614.97it/s]
 20%|â–ˆâ–‰        | 57/287 [00:03<00:09, 24.46it/s]]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 545.96it/s]
 25%|â–ˆâ–ˆâ–Œ       | 73/287 [00:04<00:15, 13.98it/s]]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 486.49it/s]
 11%|â–ˆ         | 31/287 [00:02<00:20, 12.80it/s]]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 436.74it/s]
 25%|â–ˆâ–ˆâ–       | 71/287 [00:05<00:22,  9.61it/s]]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:00<00:00, 192.05it/s]
Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loss : 0.245:   2%|â–         | 25/1276 [00:03<02:15,  9.27it/s]]]
epoch mean loss : 0.034
  2%|â–         | 29/1276 [00:02<01:55, 10.80it/s]]]
{
    "validation precision": 0.9944678026111972,
    "validation recall": 0.9944678026111972,
    "validation f1": 0.9944678026111972
}
loss : 0.014:   3%|â–Ž         | 38/1276 [00:06<07:19,  2.82it/s]]]
epoch mean loss : 0.003
 14%|â–ˆâ–Ž        | 175/1276 [00:08<00:56, 19.37it/s]]
{
    "validation precision": 0.9988933156263834,
    "validation recall": 0.9986722726266873,
    "validation f1": 0.9987827818966472
}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 831.19it/s]
 31%|â–ˆâ–ˆâ–ˆ       | 92/296 [00:04<00:08, 24.00it/s]]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 646.31it/s]
 16%|â–ˆâ–Œ        | 47/296 [00:02<00:10, 22.78it/s]]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 598.35it/s]
 23%|â–ˆâ–ˆâ–Ž       | 69/296 [00:06<00:25,  8.73it/s]]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 527.70it/s]
 22%|â–ˆâ–ˆâ–       | 66/296 [00:06<00:18, 12.16it/s]]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 473.24it/s]
  1%|          | 3/296 [00:00<00:57,  5.12it/s]]]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 422.23it/s]
  5%|â–         | 14/296 [00:01<00:18, 15.63it/s]]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:00<00:00, 468.47it/s]
Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loss : 0.222:   2%|â–         | 27/1300 [00:04<05:44,  3.69it/s]]]
epoch mean loss : 0.035
  5%|â–Œ         | 67/1300 [00:06<02:01, 10.12it/s]]]
{
    "validation precision": 0.9867428571428571,
    "validation recall": 0.9892300641613199,
    "validation f1": 0.9879848952969447
}
loss : 0.005:   2%|â–         | 22/1300 [00:04<03:12,  6.64it/s]]]
epoch mean loss : 0.003
  7%|â–‹         | 97/1300 [00:09<01:54, 10.48it/s]]]
{
    "validation precision": 0.9974753270599036,
    "validation recall": 0.9958753437213566,
    "validation f1": 0.9966746932691205
}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 905.02it/s]
 13%|â–ˆâ–Ž        | 36/273 [00:01<00:07, 30.30it/s]]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 740.44it/s]
  8%|â–Š         | 22/273 [00:01<00:14, 17.28it/s]]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 651.11it/s]
 34%|â–ˆâ–ˆâ–ˆâ–      | 93/273 [00:06<00:13, 13.31it/s]]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 507.94it/s]
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 114/273 [00:09<00:10, 15.73it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 507.62it/s]
 29%|â–ˆâ–ˆâ–‰       | 79/273 [00:09<00:19,  9.93it/s]]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 465.39it/s]
 12%|â–ˆâ–        | 34/273 [00:03<00:32,  7.46it/s]]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:00<00:00, 193.53it/s]
Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loss : 1.312:   0%|          | 4/1242 [00:00<01:43, 12.01it/s]s]]
epoch mean loss : 0.039
  7%|â–‹         | 85/1242 [00:09<01:47, 10.72it/s]]]
{
    "validation precision": 0.991751269035533,
    "validation recall": 0.9923809523809524,
    "validation f1": 0.9920660107902253
}
loss : 0.001:   0%|          | 6/1242 [00:00<01:55, 10.68it/s]]]]
epoch mean loss : 0.003
  2%|â–         | 21/1242 [00:01<01:10, 17.37it/s]]]
{
    "validation precision": 0.9949324324324325,
    "validation recall": 0.9972486772486773,
    "validation f1": 0.9960892083289293
}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 65.88it/s]
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 215/331 [00:07<00:05, 22.47it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 596.72it/s]
 29%|â–ˆâ–ˆâ–‰       | 96/331 [00:04<00:10, 21.45it/s]]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 526.70it/s]
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 120/331 [00:06<00:10, 19.63it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 481.92it/s]
 28%|â–ˆâ–ˆâ–Š       | 94/331 [00:04<00:12, 18.73it/s]]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 432.67it/s]
  4%|â–Ž         | 12/331 [00:00<00:08, 37.47it/s]]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 383.12it/s]
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 184/331 [00:06<00:03, 38.90it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:00<00:00, 191.23it/s]
Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loss : 0.052:   8%|â–Š         | 99/1186 [00:09<01:36, 11.32it/s]]]
epoch mean loss : 0.044
  1%|â–         | 17/1186 [00:00<00:58, 20.05it/s]]]
{
    "validation precision": 0.9902723735408561,
    "validation recall": 0.9888295288975231,
    "validation f1": 0.98955042527339
}
loss : 0.000:   4%|â–Ž         | 44/1186 [00:09<05:34,  3.42it/s]]]
epoch mean loss : 0.004
  8%|â–Š         | 100/1186 [00:06<01:04, 16.92it/s]]
{
    "validation precision": 0.9944201843765162,
    "validation recall": 0.9953861097620204,
    "validation f1": 0.9949029126213592
}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 65.43it/s]
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 273/387 [00:09<00:04, 24.36it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 530.75it/s]
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 129/387 [00:06<00:07, 32.44it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 459.29it/s]
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 165/387 [00:09<00:10, 20.93it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 420.13it/s]
 20%|â–ˆâ–‰        | 77/387 [00:06<01:05,  4.76it/s]]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 378.82it/s]
  5%|â–Œ         | 20/387 [00:02<01:17,  4.74it/s]]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 58.83it/s]
 20%|â–ˆâ–ˆ        | 79/387 [00:09<00:32,  9.38it/s]]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:00<00:00, 477.61it/s]
Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
loss : 0.169:   3%|â–Ž         | 42/1286 [00:03<01:43, 11.99it/s]]]
epoch mean loss : 0.038
  3%|â–Ž         | 40/1286 [00:03<01:58, 10.55it/s]]]
{
    "validation precision": 0.988761568973116,
    "validation recall": 0.9913831197525409,
    "validation f1": 0.9900706090026479
}
loss : 0.000:   1%|          | 7/1286 [00:01<03:50,  5.54it/s]]]]
epoch mean loss : 0.003
  5%|â–Œ         | 66/1286 [00:06<01:59, 10.23it/s]]]
{
    "validation precision": 0.9944885361552028,
    "validation recall": 0.9966858152894388,
    "validation f1": 0.9955859633634959
}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 803.53it/s]
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 171/287 [00:07<00:02, 44.99it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 676.69it/s]
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 96/287 [00:04<00:13, 14.59it/s]]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 617.87it/s]
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 123/287 [00:08<00:28,  5.79it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 546.98it/s]
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 130/287 [00:09<00:10, 15.43it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 477.04it/s]
 32%|â–ˆâ–ˆâ–ˆâ–      | 92/287 [00:03<00:11, 17.22it/s]]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 434.96it/s]
 13%|â–ˆâ–Ž        | 38/287 [00:01<00:08, 28.31it/s]]
INFO - xp_kfolds_list - Completed after 2:50:25
